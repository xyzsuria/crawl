{"What Makes Good In-Context Examples for GPT-?": "https://arxiv.org/pdf/2101.06804", "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings": "https://arxiv.org/pdf/1909.00512", "GPT understands, too": "https://arxiv.org/pdf/2103.10385", "Sparks of artificial general intelligence: Early experiments with gpt-4": "https://arxiv.org/pdf/2303.12712", "Hello, it's GPT-2--how can I help you? towards the use of pretrained language models for task-oriented dialogue systems": "https://arxiv.org/pdf/1907.05774", "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?": "https://arxiv.org/pdf/2303.11717", "Gpt-neox-20b: An open-source autoregressive language model": "https://arxiv.org/pdf/2204.06745", "The radicalization risks of GPT-3 and advanced neural language models": "https://arxiv.org/pdf/2009.06807.pdf),", "GPT-too: A language-model-first approach for AMR-to-text generation": "https://arxiv.org/pdf/2005.09123", "How good are gpt models at machine translation? a comprehensive evaluation": "https://arxiv.org/pdf/2302.09210", "Want to reduce labeling cost? GPT-3 can help": "https://arxiv.org/pdf/2108.13487", "Locating and editing factual knowledge in gpt": "https://arxiv.org/pdf/2202.05262.pdf%C3%82%C2%A0", "Instruction tuning with gpt-4": "https://arxiv.org/pdf/2304.03277", "Automatic text summarization of covid-19 medical research articles using bert and gpt-2": "https://arxiv.org/pdf/2006.01997", "Learning to answer by learning to ask: Getting the best of gpt-2 and bert worlds": "https://arxiv.org/pdf/1911.02365", "Detecting hate speech with gpt-3": "https://arxiv.org/pdf/2103.12407", "Is GPT-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text": "https://arxiv.org/pdf/2107.01294"}