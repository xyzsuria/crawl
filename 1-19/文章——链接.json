{"What Makes Good In-Context Examples for GPT-?": "https://arxiv.org/pdf/2101.06804", "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings": "https://arxiv.org/pdf/1909.00512", "GPT understands, too": "https://arxiv.org/pdf/2103.10385", "Hello, it's GPT-2--how can I help you? towards the use of pretrained language models for task-oriented dialogue systems": "https://arxiv.org/pdf/1907.05774", "Sparks of artificial general intelligence: Early experiments with gpt-4": "https://arxiv.org/pdf/2303.12712", "Gpt-neox-20b: An open-source autoregressive language model": "https://arxiv.org/pdf/2204.06745", "The radicalization risks of GPT-3 and advanced neural language models": "https://arxiv.org/pdf/2009.06807.pdf),", "Patent claim generation by fine-tuning OpenAI GPT-2": "https://arxiv.org/pdf/1907.02052", "As good as new. how to successfully recycle english gpt-2 to make models for other languages": "https://arxiv.org/pdf/2012.05628", "Capabilities of gpt-4 on medical challenge problems": "https://arxiv.org/pdf/2303.13375", "How good are gpt models at machine translation? a comprehensive evaluation": "https://arxiv.org/pdf/2302.09210", "Want to reduce labeling cost? GPT-3 can help": "https://arxiv.org/pdf/2108.13487", "Exploring transformers in natural language generation: Gpt, bert, and xlnet": "https://arxiv.org/pdf/2102.08036", "Instruction tuning with gpt-4": "https://arxiv.org/pdf/2304.03277", "Sgpt: Gpt sentence embeddings for semantic search": "https://arxiv.org/pdf/2202.08904", "Automatic text summarization of covid-19 medical research articles using bert and gpt-2": "https://arxiv.org/pdf/2006.01997", "Gpt-based generation for classical chinese poetry": "https://arxiv.org/pdf/1907.00151", "Detecting hate speech with gpt-3": "https://arxiv.org/pdf/2103.12407", "Who is GPT-3? An exploration of personality, values and demographics": "https://arxiv.org/pdf/2209.14338", "Gpt-3 models are poor few-shot learners in the biomedical domain": "https://arxiv.org/pdf/2109.02555", "Chat2vis: Generating data visualisations via natural language using chatgpt, codex and gpt-3 large language models": "https://arxiv.org/pdf/2302.02094", "Assessing discourse relations in language generation from GPT-2": "https://arxiv.org/pdf/2004.12506", "Prompting gpt-3 to be reliable": "https://arxiv.org/pdf/2210.09150", "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks": "https://arxiv.org/pdf/2303.00293", "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers": "https://arxiv.org/pdf/2212.10559", "Summary of chatgpt/gpt-4 research and perspective towards the future of large language models": "https://arxiv.org/pdf/2304.01852", "Variational latent-state GPT for semi-supervised task-oriented dialog systems": "https://arxiv.org/pdf/2109.04314", "Thinking aloud: Dynamic context generation improves zero-shot reasoning performance of gpt-2": "https://arxiv.org/pdf/2103.13033", "GPT3-to-plan: Extracting plans from text using GPT-3": "https://arxiv.org/pdf/2106.07131", "Is GPT-3 All You Need for Visual Question Answering in Cultural Heritage?": "https://arxiv.org/pdf/2207.12101", "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?": "https://arxiv.org/pdf/2303.09325", "Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: Promising results, limitations, and potential": "https://arxiv.org/pdf/2303.09038", "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance": "https://arxiv.org/pdf/2303.16894", "Memory-assisted prompt editing to improve GPT-3 after deployment": "https://arxiv.org/pdf/2201.06009", "Adavae: Exploring adaptive GPT-2s in variational auto-encoders for language modeling": "https://arxiv.org/pdf/2205.05862", "Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code": "https://arxiv.org/pdf/2303.08033", "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small": "https://arxiv.org/pdf/2211.00593", "Can GPT-3 perform statutory reasoning?": "https://arxiv.org/pdf/2302.06100", "What gpt knows about who is who": "https://arxiv.org/pdf/2205.07407", "Leveraging GPT-2 for classifying spam reviews with limited labeled data via adversarial training": "https://arxiv.org/pdf/2012.13400", "Ask me what you need: Product retrieval using knowledge from gpt-3": "https://arxiv.org/pdf/2207.02516", "GPT Takes the Bar Exam": "https://arxiv.org/pdf/2212.14402", "Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements": "https://arxiv.org/pdf/2205.11374", "Fine-tuning gpt-3 for russian text summarization": "https://arxiv.org/pdf/2108.03502", "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models": "https://arxiv.org/pdf/2303.10420", "Modern french poetry generation with roberta and gpt-2": "https://arxiv.org/pdf/2212.02911", "Twisted Gabidulin codes in the GPT cryptosystem": "https://arxiv.org/pdf/1806.10055", "CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex": "https://arxiv.org/pdf/2204.08941", "GPT-3-driven pedagogical agents for training children's curious question-asking skills": "https://arxiv.org/pdf/2211.14228", "Machine intuition: Uncovering human-like intuitive decision-making in GPT-3.5": "https://arxiv.org/pdf/2212.05206", "Heroes, Villains, and Victims, and GPT-3--Automated Extraction of Character Roles Without Training Data": "https://arxiv.org/pdf/2205.07557"}